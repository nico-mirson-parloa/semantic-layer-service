{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Semantic Layer SQL API - Jupyter Notebook Demo\n",
        "\n",
        "This notebook demonstrates how to connect to the Semantic Layer Service using the SQL API from Jupyter notebooks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Connection\n",
        "\n",
        "First, let's install and import the required packages for connecting to the Semantic Layer SQL API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages if needed\n",
        "# !pip install psycopg2-binary pandas matplotlib seaborn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import psycopg2\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect to Semantic Layer SQL API\n",
        "conn = psycopg2.connect(\n",
        "    host=\"localhost\",\n",
        "    port=5433,\n",
        "    database=\"semantic_layer\",\n",
        "    user=\"jupyter_user\"\n",
        ")\n",
        "\n",
        "print(\"âœ… Connected to Semantic Layer\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Explore Available Semantic Models\n",
        "\n",
        "Let's discover what semantic models are available in our semantic layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all semantic models\n",
        "query = \"\"\"\n",
        "SELECT \n",
        "    schema_name,\n",
        "    REPLACE(schema_name, 'sem_', '') as model_name\n",
        "FROM information_schema.schemata \n",
        "WHERE schema_name LIKE 'sem_%'\n",
        "ORDER BY schema_name\n",
        "\"\"\"\n",
        "\n",
        "models_df = pd.read_sql_query(query, conn)\n",
        "models_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get tables for first model\n",
        "if not models_df.empty:\n",
        "    schema = models_df.iloc[0]['schema_name']\n",
        "    \n",
        "    query = f\"\"\"\n",
        "    SELECT \n",
        "        table_name,\n",
        "        table_type,\n",
        "        (SELECT COUNT(*) \n",
        "         FROM information_schema.columns c \n",
        "         WHERE c.table_schema = t.table_schema \n",
        "         AND c.table_name = t.table_name) as column_count\n",
        "    FROM information_schema.tables t\n",
        "    WHERE table_schema = '{schema}'\n",
        "    ORDER BY table_name\n",
        "    \"\"\"\n",
        "    \n",
        "    tables_df = pd.read_sql_query(query, conn)\n",
        "    print(f\"Tables in {schema}:\")\n",
        "    tables_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Query Semantic Data\n",
        "\n",
        "Now let's query actual data from our semantic models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample data from fact table\n",
        "if not models_df.empty:\n",
        "    schema = models_df.iloc[0]['schema_name']\n",
        "    \n",
        "    query = f\"SELECT * FROM {schema}.fact LIMIT 10\"\n",
        "    \n",
        "    sample_df = pd.read_sql_query(query, conn)\n",
        "    print(f\"Sample data from {schema}.fact:\")\n",
        "    sample_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Business Analytics Queries\n",
        "\n",
        "Let's run some typical business analytics queries. Note: You may need to adjust column names based on your specific semantic model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Time series analysis\n",
        "# Note: Adjust column names based on your semantic model\n",
        "\n",
        "time_series_query = f\"\"\"\n",
        "SELECT \n",
        "    DATE_TRUNC('month', order_date) as month,\n",
        "    SUM(revenue) as monthly_revenue,\n",
        "    COUNT(DISTINCT customer_id) as unique_customers,\n",
        "    COUNT(*) as transaction_count\n",
        "FROM {schema}.fact\n",
        "WHERE order_date >= CURRENT_DATE - INTERVAL '12 months'\n",
        "GROUP BY 1\n",
        "ORDER BY 1\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    time_series_df = pd.read_sql_query(time_series_query, conn)\n",
        "    time_series_df['month'] = pd.to_datetime(time_series_df['month'])\n",
        "    time_series_df.set_index('month', inplace=True)\n",
        "    \n",
        "    # Plot revenue trend\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    time_series_df['monthly_revenue'].plot(ax=ax, marker='o', linewidth=2)\n",
        "    ax.set_title('Monthly Revenue Trend', fontsize=16)\n",
        "    ax.set_xlabel('Month')\n",
        "    ax.set_ylabel('Revenue')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Query failed: {e}\")\n",
        "    print(\"Please adjust column names to match your semantic model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Top categories analysis\n",
        "# Note: Adjust column names based on your semantic model\n",
        "\n",
        "category_query = f\"\"\"\n",
        "SELECT \n",
        "    category,\n",
        "    SUM(revenue) as total_revenue,\n",
        "    COUNT(DISTINCT customer_id) as customers,\n",
        "    AVG(revenue) as avg_order_value\n",
        "FROM {schema}.fact\n",
        "GROUP BY category\n",
        "ORDER BY total_revenue DESC\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    category_df = pd.read_sql_query(category_query, conn)\n",
        "    \n",
        "    # Create horizontal bar chart\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    category_df.plot.barh(x='category', y='total_revenue', ax=ax, color='skyblue')\n",
        "    ax.set_title('Top Categories by Revenue', fontsize=16)\n",
        "    ax.set_xlabel('Total Revenue')\n",
        "    ax.set_ylabel('Category')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Display the data\n",
        "    category_df\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Query failed: {e}\")\n",
        "    print(\"Please adjust column names (e.g., 'category') to match your semantic model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Advanced Analytics with Pandas\n",
        "\n",
        "Let's do some more advanced analytics by combining SQL queries with pandas operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Cohort Analysis\n",
        "# Get customer first purchase date and subsequent behavior\n",
        "\n",
        "cohort_query = f\"\"\"\n",
        "WITH customer_cohorts AS (\n",
        "    SELECT \n",
        "        customer_id,\n",
        "        DATE_TRUNC('month', MIN(order_date)) as cohort_month,\n",
        "        DATE_TRUNC('month', order_date) as order_month,\n",
        "        SUM(revenue) as revenue\n",
        "    FROM {schema}.fact\n",
        "    GROUP BY customer_id, DATE_TRUNC('month', order_date)\n",
        ")\n",
        "SELECT \n",
        "    cohort_month,\n",
        "    order_month,\n",
        "    COUNT(DISTINCT customer_id) as customers,\n",
        "    SUM(revenue) as total_revenue\n",
        "FROM customer_cohorts\n",
        "GROUP BY cohort_month, order_month\n",
        "ORDER BY cohort_month, order_month\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    cohort_df = pd.read_sql_query(cohort_query, conn)\n",
        "    \n",
        "    # Calculate months since first purchase\n",
        "    cohort_df['cohort_month'] = pd.to_datetime(cohort_df['cohort_month'])\n",
        "    cohort_df['order_month'] = pd.to_datetime(cohort_df['order_month'])\n",
        "    cohort_df['months_since_cohort'] = (\n",
        "        (cohort_df['order_month'] - cohort_df['cohort_month']) / pd.Timedelta(days=30)\n",
        "    ).round().astype(int)\n",
        "    \n",
        "    # Create cohort retention matrix\n",
        "    cohort_pivot = cohort_df.pivot_table(\n",
        "        index='cohort_month',\n",
        "        columns='months_since_cohort',\n",
        "        values='customers',\n",
        "        aggfunc='sum'\n",
        "    )\n",
        "    \n",
        "    # Calculate retention rates\n",
        "    cohort_retention = cohort_pivot.divide(cohort_pivot[0], axis=0) * 100\n",
        "    \n",
        "    # Plot retention heatmap\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(cohort_retention.iloc[:6, :6], \n",
        "                annot=True, \n",
        "                fmt='.1f', \n",
        "                cmap='YlOrRd', \n",
        "                cbar_kws={'label': 'Retention %'})\n",
        "    plt.title('Customer Retention by Cohort', fontsize=16)\n",
        "    plt.xlabel('Months Since First Purchase')\n",
        "    plt.ylabel('Cohort Month')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Cohort analysis failed: {e}\")\n",
        "    print(\"This example requires customer_id and order_date columns\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Working with Metric Views\n",
        "\n",
        "The Semantic Layer may provide pre-calculated metric views for common analytics needs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for metric views\n",
        "if not models_df.empty:\n",
        "    schema = models_df.iloc[0]['schema_name']\n",
        "    \n",
        "    view_query = f\"\"\"\n",
        "    SELECT \n",
        "        table_name as view_name,\n",
        "        table_type\n",
        "    FROM information_schema.tables\n",
        "    WHERE table_schema = '{schema}'\n",
        "    AND table_type = 'VIEW'\n",
        "    ORDER BY table_name\n",
        "    \"\"\"\n",
        "    \n",
        "    views_df = pd.read_sql_query(view_query, conn)\n",
        "    \n",
        "    if not views_df.empty:\n",
        "        print(f\"Available metric views in {schema}:\")\n",
        "        for view in views_df['view_name']:\n",
        "            print(f\"  - {view}\")\n",
        "            \n",
        "        # Query a metric view\n",
        "        metric_view = views_df.iloc[0]['view_name']\n",
        "        sample_query = f\"SELECT * FROM {schema}.{metric_view} LIMIT 5\"\n",
        "        \n",
        "        print(f\"\\nSample from {metric_view}:\")\n",
        "        pd.read_sql_query(sample_query, conn)\n",
        "    else:\n",
        "        print(\"No metric views found in this semantic model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Exploring Column Metadata\n",
        "\n",
        "Let's explore the columns and their semantic types to better understand our data model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get column information for fact table\n",
        "if not models_df.empty:\n",
        "    schema = models_df.iloc[0]['schema_name']\n",
        "    \n",
        "    columns_query = f\"\"\"\n",
        "    SELECT \n",
        "        column_name,\n",
        "        data_type,\n",
        "        is_nullable,\n",
        "        column_default,\n",
        "        -- Extract semantic type from column comment if available\n",
        "        CASE \n",
        "            WHEN col_description(pgc.oid, a.attnum) LIKE '%dimension%' THEN 'dimension'\n",
        "            WHEN col_description(pgc.oid, a.attnum) LIKE '%measure%' THEN 'measure'\n",
        "            WHEN col_description(pgc.oid, a.attnum) LIKE '%entity%' THEN 'entity'\n",
        "            ELSE 'unknown'\n",
        "        END as semantic_type\n",
        "    FROM information_schema.columns c\n",
        "    LEFT JOIN pg_catalog.pg_attribute a ON a.attname = c.column_name\n",
        "    LEFT JOIN pg_catalog.pg_class pgc ON pgc.relname = c.table_name\n",
        "    WHERE c.table_schema = '{schema}'\n",
        "    AND c.table_name = 'fact'\n",
        "    AND a.attnum > 0\n",
        "    AND NOT a.attisdropped\n",
        "    ORDER BY c.ordinal_position\n",
        "    \"\"\"\n",
        "    \n",
        "    columns_df = pd.read_sql_query(columns_query, conn)\n",
        "    \n",
        "    print(f\"Columns in {schema}.fact:\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    # Group by semantic type\n",
        "    for sem_type in ['dimension', 'measure', 'entity', 'unknown']:\n",
        "        type_cols = columns_df[columns_df['semantic_type'] == sem_type]\n",
        "        if not type_cols.empty:\n",
        "            print(f\"\\n{sem_type.upper()}S:\")\n",
        "            for _, col in type_cols.iterrows():\n",
        "                print(f\"  - {col['column_name']} ({col['data_type']})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Performance Tips and Best Practices\n",
        "\n",
        "When working with the Semantic Layer SQL API, here are some tips for optimal performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Using EXPLAIN to understand query performance\n",
        "if not models_df.empty:\n",
        "    schema = models_df.iloc[0]['schema_name']\n",
        "    \n",
        "    # Create a test query\n",
        "    test_query = f\"\"\"\n",
        "    SELECT \n",
        "        DATE_TRUNC('month', order_date) as month,\n",
        "        SUM(revenue) as total_revenue\n",
        "    FROM {schema}.fact\n",
        "    WHERE order_date >= CURRENT_DATE - INTERVAL '12 months'\n",
        "    GROUP BY 1\n",
        "    \"\"\"\n",
        "    \n",
        "    # Get query plan\n",
        "    explain_query = f\"EXPLAIN (ANALYZE FALSE, FORMAT JSON) {test_query}\"\n",
        "    \n",
        "    try:\n",
        "        explain_df = pd.read_sql_query(explain_query, conn)\n",
        "        plan = explain_df.iloc[0, 0][0]['Plan']\n",
        "        \n",
        "        print(\"Query Execution Plan:\")\n",
        "        print(\"-\" * 60)\n",
        "        print(f\"Node Type: {plan.get('Node Type')}\")\n",
        "        print(f\"Strategy: {plan.get('Strategy', 'N/A')}\")\n",
        "        print(f\"Startup Cost: {plan.get('Startup Cost', 'N/A')}\")\n",
        "        print(f\"Total Cost: {plan.get('Total Cost', 'N/A')}\")\n",
        "        print(f\"Plan Rows: {plan.get('Plan Rows', 'N/A')}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Could not get query plan: {e}\")\n",
        "\n",
        "# Performance best practices\n",
        "print(\"\\nðŸ“Š Performance Best Practices:\")\n",
        "print(\"-\" * 60)\n",
        "print(\"1. Use date filters to limit data scanned\")\n",
        "print(\"2. Aggregate at the database level, not in pandas\")\n",
        "print(\"3. Select only needed columns\")\n",
        "print(\"4. Use proper indexes (handled by semantic layer)\")\n",
        "print(\"5. Batch multiple queries when possible\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Error Handling and Debugging\n",
        "\n",
        "Proper error handling is important when working with database connections.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Robust query execution with error handling\n",
        "def safe_query(connection, query, params=None):\n",
        "    \"\"\"Execute a query with proper error handling.\"\"\"\n",
        "    try:\n",
        "        # Create a new cursor for this query\n",
        "        with connection.cursor() as cursor:\n",
        "            cursor.execute(query, params)\n",
        "            \n",
        "            # For SELECT queries, fetch results\n",
        "            if cursor.description:\n",
        "                columns = [desc[0] for desc in cursor.description]\n",
        "                results = cursor.fetchall()\n",
        "                return pd.DataFrame(results, columns=columns)\n",
        "            else:\n",
        "                # For INSERT/UPDATE/DELETE\n",
        "                connection.commit()\n",
        "                return f\"Query executed successfully. Rows affected: {cursor.rowcount}\"\n",
        "                \n",
        "    except psycopg2.Error as e:\n",
        "        # Rollback on error\n",
        "        connection.rollback()\n",
        "        print(f\"Database error: {e.pgerror}\")\n",
        "        print(f\"Error code: {e.pgcode}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {type(e).__name__}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Test with an invalid query\n",
        "invalid_query = \"SELECT * FROM non_existent_table\"\n",
        "result = safe_query(conn, invalid_query)\n",
        "\n",
        "# Test with a valid query\n",
        "if not models_df.empty:\n",
        "    valid_query = f\"SELECT COUNT(*) as row_count FROM {models_df.iloc[0]['schema_name']}.fact\"\n",
        "    result = safe_query(conn, valid_query)\n",
        "    if result is not None:\n",
        "        print(f\"âœ… Valid query result: {result.iloc[0]['row_count']} rows\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Advanced Use Cases\n",
        "\n",
        "Here are some advanced analytics examples you can perform with the Semantic Layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Customer Lifetime Value Analysis\n",
        "# This combines multiple analytical techniques\n",
        "\n",
        "if not models_df.empty:\n",
        "    schema = models_df.iloc[0]['schema_name']\n",
        "    \n",
        "    clv_query = f\"\"\"\n",
        "    WITH customer_metrics AS (\n",
        "        SELECT \n",
        "            customer_id,\n",
        "            MIN(order_date) as first_purchase_date,\n",
        "            MAX(order_date) as last_purchase_date,\n",
        "            COUNT(DISTINCT order_date) as purchase_count,\n",
        "            SUM(revenue) as total_revenue,\n",
        "            AVG(revenue) as avg_order_value,\n",
        "            -- Calculate days between first and last purchase\n",
        "            EXTRACT(DAY FROM MAX(order_date) - MIN(order_date)) as customer_lifespan_days\n",
        "        FROM {schema}.fact\n",
        "        GROUP BY customer_id\n",
        "    ),\n",
        "    customer_segments AS (\n",
        "        SELECT \n",
        "            *,\n",
        "            CASE \n",
        "                WHEN total_revenue >= (SELECT PERCENTILE_CONT(0.8) WITHIN GROUP (ORDER BY total_revenue) FROM customer_metrics) THEN 'High Value'\n",
        "                WHEN total_revenue >= (SELECT PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY total_revenue) FROM customer_metrics) THEN 'Medium Value'\n",
        "                ELSE 'Low Value'\n",
        "            END as value_segment,\n",
        "            CASE\n",
        "                WHEN purchase_count >= 5 THEN 'Frequent'\n",
        "                WHEN purchase_count >= 2 THEN 'Occasional'\n",
        "                ELSE 'One-time'\n",
        "            END as frequency_segment\n",
        "        FROM customer_metrics\n",
        "    )\n",
        "    SELECT \n",
        "        value_segment,\n",
        "        frequency_segment,\n",
        "        COUNT(*) as customer_count,\n",
        "        AVG(total_revenue) as avg_lifetime_value,\n",
        "        AVG(purchase_count) as avg_purchases,\n",
        "        AVG(customer_lifespan_days) as avg_lifespan_days\n",
        "    FROM customer_segments\n",
        "    GROUP BY value_segment, frequency_segment\n",
        "    ORDER BY value_segment, frequency_segment\n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        clv_df = pd.read_sql_query(clv_query, conn)\n",
        "        \n",
        "        # Create a heatmap of customer segments\n",
        "        pivot_df = clv_df.pivot(index='value_segment', \n",
        "                                columns='frequency_segment', \n",
        "                                values='customer_count')\n",
        "        \n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.heatmap(pivot_df, annot=True, fmt='g', cmap='Blues')\n",
        "        plt.title('Customer Segmentation Matrix', fontsize=16)\n",
        "        plt.ylabel('Value Segment')\n",
        "        plt.xlabel('Frequency Segment')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Display segment metrics\n",
        "        print(\"Customer Segment Analysis:\")\n",
        "        clv_df\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"CLV analysis failed: {e}\")\n",
        "        print(\"This example requires customer_id, order_date, and revenue columns\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
