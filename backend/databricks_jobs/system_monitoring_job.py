"""
Databricks Job: System Monitoring and Health Checks

This job runs every 5 minutes to:
1. Monitor system health and performance
2. Check volume accessibility
3. Monitor query execution times
4. Send alerts for issues
5. Update monitoring dashboards

Deploy this as a Databricks Job with:
- Cluster: Single node with Databricks Runtime
- Schedule: Every 5 minutes  
- Libraries: databricks-sdk, requests
"""

import os
import json
import time
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional

import structlog
from databricks.sdk import WorkspaceClient
from pyspark.sql import SparkSession

# Configure logging
structlog.configure(
    processors=[structlog.dev.ConsoleRenderer()],
    wrapper_class=structlog.make_filtering_bound_logger(20),
    logger_factory=structlog.PrintLoggerFactory(),
)
logger = structlog.get_logger(__name__)


class SystemMonitor:
    """Monitor semantic layer system health and performance."""
    
    def __init__(self):
        self.client = WorkspaceClient()
        self.spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()
        self.volume_base_path = "/Volumes/semantic_layer/metrics"
        
        # Ensure monitoring tables exist
        self._ensure_monitoring_tables()
    
    def _ensure_monitoring_tables(self):
        """Ensure monitoring tables exist."""
        try:
            # Create monitoring schema
            self.spark.sql("""
                CREATE SCHEMA IF NOT EXISTS semantic_layer.monitoring
                COMMENT 'System monitoring and health check data'
            """)
            
            # System health table
            self.spark.sql("""
                CREATE TABLE IF NOT EXISTS semantic_layer.monitoring.system_health (
                    timestamp TIMESTAMP,
                    component STRING,
                    status STRING,
                    response_time_ms BIGINT,
                    error_message STRING,
                    details MAP<STRING, STRING>
                ) USING DELTA
                PARTITIONED BY (date(timestamp))
                TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true')
            """)
            
            # Volume health table  
            self.spark.sql("""
                CREATE TABLE IF NOT EXISTS semantic_layer.monitoring.volume_health (
                    timestamp TIMESTAMP,
                    volume_path STRING,
                    accessible BOOLEAN,
                    file_count BIGINT,
                    total_size_bytes BIGINT,
                    last_modified TIMESTAMP,
                    error_message STRING
                ) USING DELTA
                PARTITIONED BY (date(timestamp))
                TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true')
            """)
            
            # Performance metrics table
            self.spark.sql("""
                CREATE TABLE IF NOT EXISTS semantic_layer.monitoring.performance_metrics (
                    timestamp TIMESTAMP,
                    metric_type STRING,
                    metric_name STRING,
                    value DOUBLE,
                    unit STRING,
                    dimensions MAP<STRING, STRING>
                ) USING DELTA
                PARTITIONED BY (date(timestamp))
                TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true')
            """)
            
            logger.info("Monitoring tables initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize monitoring tables: {e}")
            raise
    
    def check_volume_health(self) -> Dict[str, Any]:
        """Check health of all semantic layer volumes."""\n        volume_health = {}\n        \n        categories = [\"production_models\", \"staging_models\", \"templates\", \"archives\"]\n        \n        for category in categories:\n            try:\n                volume_path = f\"{self.volume_base_path}/{category}\"\n                \n                start_time = time.time()\n                \n                # Try to list files in volume\n                files = self.client.files.list_directory_contents(directory_path=volume_path)\n                file_list = list(files)  # Convert to list to get count\n                \n                response_time = int((time.time() - start_time) * 1000)\n                \n                # Calculate total size and find last modified\n                total_size = 0\n                last_modified = None\n                \n                for file_info in file_list:\n                    if hasattr(file_info, 'size'):\n                        total_size += file_info.size or 0\n                    if hasattr(file_info, 'last_modified'):\n                        if not last_modified or (file_info.last_modified and file_info.last_modified > last_modified):\n                            last_modified = file_info.last_modified\n                \n                volume_health[category] = {\n                    \"accessible\": True,\n                    \"file_count\": len(file_list),\n                    \"total_size_bytes\": total_size,\n                    \"response_time_ms\": response_time,\n                    \"last_modified\": last_modified,\n                    \"error_message\": None\n                }\n                \n                # Record in monitoring table\n                self.spark.sql(f\"\"\"\n                    INSERT INTO semantic_layer.monitoring.volume_health\n                    VALUES (\n                        current_timestamp(),\n                        '{volume_path}',\n                        true,\n                        {len(file_list)},\n                        {total_size},\n                        {'null' if not last_modified else f\"timestamp '{last_modified.strftime('%Y-%m-%d %H:%M:%S')}'\" if hasattr(last_modified, 'strftime') else 'null'},\n                        null\n                    )\n                \"\"\")\n                \n                logger.info(f\"Volume {category} is healthy\", file_count=len(file_list))\n                \n            except Exception as e:\n                error_msg = str(e)\n                volume_health[category] = {\n                    \"accessible\": False,\n                    \"file_count\": 0,\n                    \"total_size_bytes\": 0,\n                    \"response_time_ms\": 0,\n                    \"last_modified\": None,\n                    \"error_message\": error_msg\n                }\n                \n                # Record error in monitoring table\n                self.spark.sql(f\"\"\"\n                    INSERT INTO semantic_layer.monitoring.volume_health\n                    VALUES (\n                        current_timestamp(),\n                        '{self.volume_base_path}/{category}',\n                        false,\n                        0,\n                        0,\n                        null,\n                        '{error_msg.replace(\"'\", \"''\")}'  \n                    )\n                \"\"\")\n                \n                logger.error(f\"Volume {category} is not accessible\", error=error_msg)\n        \n        return volume_health\n    \n    def check_cache_performance(self) -> Dict[str, Any]:\n        \"\"\"Check cache performance metrics.\"\"\"\n        try:\n            # Get cache hit rate for last hour\n            cache_stats = self.spark.sql(\"\"\"\n                SELECT \n                    COUNT(*) as total_requests,\n                    SUM(CASE WHEN hit THEN 1 ELSE 0 END) as cache_hits,\n                    AVG(execution_time_ms) as avg_execution_time,\n                    COUNT(DISTINCT metric_name) as unique_metrics\n                FROM semantic_layer.cache.performance_metrics\n                WHERE timestamp > current_timestamp() - INTERVAL 1 HOUR\n            \"\"\").collect()[0].asDict()\n            \n            hit_rate = 0.0\n            if cache_stats['total_requests'] and cache_stats['total_requests'] > 0:\n                hit_rate = cache_stats['cache_hits'] / cache_stats['total_requests']\n            \n            # Get cache table stats\n            cache_table_stats = self.spark.sql(\"\"\"\n                SELECT \n                    COUNT(*) as total_cached_entries,\n                    COUNT(CASE WHEN expires_at > current_timestamp() THEN 1 END) as active_entries,\n                    SUM(hit_count) as total_hits_all_time,\n                    AVG(LENGTH(result_data)) as avg_result_size\n                FROM semantic_layer.cache.query_results\n            \"\"\").collect()[0].asDict()\n            \n            performance = {\n                \"hit_rate\": hit_rate,\n                \"total_requests_1h\": cache_stats['total_requests'] or 0,\n                \"avg_execution_time_ms\": cache_stats['avg_execution_time'] or 0,\n                \"unique_metrics_1h\": cache_stats['unique_metrics'] or 0,\n                \"total_cached_entries\": cache_table_stats['total_cached_entries'] or 0,\n                \"active_entries\": cache_table_stats['active_entries'] or 0,\n                \"total_hits_all_time\": cache_table_stats['total_hits_all_time'] or 0,\n                \"avg_result_size_bytes\": cache_table_stats['avg_result_size'] or 0\n            }\n            \n            # Record performance metrics\n            timestamp = datetime.now()\n            for metric_name, value in performance.items():\n                self.spark.sql(f\"\"\"\n                    INSERT INTO semantic_layer.monitoring.performance_metrics\n                    VALUES (\n                        timestamp '{timestamp.strftime('%Y-%m-%d %H:%M:%S')}',\n                        'cache',\n                        '{metric_name}',\n                        {value},\n                        '{'ratio' if 'rate' in metric_name else 'count' if 'total' in metric_name or 'entries' in metric_name else 'ms' if 'time' in metric_name else 'bytes' if 'size' in metric_name else 'number'}',\n                        map()\n                    )\n                \"\"\")\n            \n            return performance\n            \n        except Exception as e:\n            logger.error(f\"Failed to check cache performance: {e}\")\n            return {}\n    \n    def check_query_execution_health(self) -> Dict[str, Any]:\n        \"\"\"Check recent query execution health.\"\"\"\n        try:\n            # Test a simple query execution\n            start_time = time.time()\n            \n            test_result = self.spark.sql(\"SELECT current_timestamp() as test_time\").collect()\n            \n            execution_time = int((time.time() - start_time) * 1000)\n            \n            health = {\n                \"query_execution_available\": True,\n                \"test_query_time_ms\": execution_time,\n                \"test_result\": str(test_result[0]['test_time']) if test_result else None,\n                \"error_message\": None\n            }\n            \n            # Record system health\n            self.spark.sql(f\"\"\"\n                INSERT INTO semantic_layer.monitoring.system_health\n                VALUES (\n                    current_timestamp(),\n                    'query_execution',\n                    'healthy',\n                    {execution_time},\n                    null,\n                    map('test_query', 'SELECT current_timestamp()')\n                )\n            \"\"\")\n            \n            return health\n            \n        except Exception as e:\n            error_msg = str(e)\n            \n            health = {\n                \"query_execution_available\": False,\n                \"test_query_time_ms\": 0,\n                \"test_result\": None,\n                \"error_message\": error_msg\n            }\n            \n            # Record system health error\n            self.spark.sql(f\"\"\"\n                INSERT INTO semantic_layer.monitoring.system_health\n                VALUES (\n                    current_timestamp(),\n                    'query_execution',\n                    'error',\n                    0,\n                    '{error_msg.replace(\"'\", \"''\")}',\n                    map()\n                )\n            \"\"\")\n            \n            return health\n    \n    def check_databricks_genie_health(self) -> Dict[str, Any]:\n        \"\"\"Check Databricks Genie API health.\"\"\"\n        try:\n            # Simple health check by trying to list workspaces or similar\n            start_time = time.time()\n            \n            # Test Databricks API connectivity\n            current_user = self.client.current_user.me()\n            \n            response_time = int((time.time() - start_time) * 1000)\n            \n            health = {\n                \"genie_api_available\": True,\n                \"api_response_time_ms\": response_time,\n                \"current_user\": current_user.user_name if current_user else None,\n                \"error_message\": None\n            }\n            \n            # Record system health\n            self.spark.sql(f\"\"\"\n                INSERT INTO semantic_layer.monitoring.system_health\n                VALUES (\n                    current_timestamp(),\n                    'genie_api',\n                    'healthy',\n                    {response_time},\n                    null,\n                    map('user', '{current_user.user_name if current_user else 'unknown'}')\n                )\n            \"\"\")\n            \n            return health\n            \n        except Exception as e:\n            error_msg = str(e)\n            \n            health = {\n                \"genie_api_available\": False,\n                \"api_response_time_ms\": 0,\n                \"current_user\": None,\n                \"error_message\": error_msg\n            }\n            \n            # Record system health error\n            self.spark.sql(f\"\"\"\n                INSERT INTO semantic_layer.monitoring.system_health\n                VALUES (\n                    current_timestamp(),\n                    'genie_api',\n                    'error',\n                    0,\n                    '{error_msg.replace(\"'\", \"''\")}',\n                    map()\n                )\n            \"\"\")\n            \n            return health\n    \n    def get_system_summary(self) -> Dict[str, Any]:\n        \"\"\"Get overall system health summary.\"\"\"\n        try:\n            # Get recent error counts\n            error_summary = self.spark.sql(\"\"\"\n                SELECT \n                    component,\n                    COUNT(*) as error_count,\n                    MAX(timestamp) as last_error\n                FROM semantic_layer.monitoring.system_health\n                WHERE timestamp > current_timestamp() - INTERVAL 1 HOUR\n                  AND status = 'error'\n                GROUP BY component\n            \"\"\").collect()\n            \n            errors_by_component = {}\n            for row in error_summary:\n                errors_by_component[row['component']] = {\n                    \"error_count\": row['error_count'],\n                    \"last_error\": row['last_error']\n                }\n            \n            # Get cache performance summary\n            cache_summary = self.check_cache_performance()\n            \n            return {\n                \"timestamp\": datetime.now().isoformat(),\n                \"overall_status\": \"healthy\" if not errors_by_component else \"degraded\",\n                \"errors_by_component\": errors_by_component,\n                \"cache_performance\": cache_summary\n            }\n            \n        except Exception as e:\n            logger.error(f\"Failed to get system summary: {e}\")\n            return {\n                \"timestamp\": datetime.now().isoformat(),\n                \"overall_status\": \"unknown\",\n                \"error\": str(e)\n            }\n    \n    def send_slack_alert(self, message: str, severity: str = \"info\"):\n        \"\"\"Send Slack alert.\"\"\"\n        try:\n            import requests\n            \n            webhook_url = os.getenv('SLACK_WEBHOOK_URL')\n            if not webhook_url:\n                logger.info(f\"Slack alert (no webhook): {message}\")\n                return\n            \n            color_map = {\n                \"info\": \"#36a64f\",\n                \"warning\": \"#ff9500\",\n                \"error\": \"#ff0000\",\n                \"critical\": \"#ff0000\"\n            }\n            \n            payload = {\n                \"attachments\": [{\n                    \"color\": color_map.get(severity, \"#36a64f\"),\n                    \"text\": f\"üîç **Semantic Layer Monitoring**\\n{message}\",\n                    \"ts\": time.time()\n                }]\n            }\n            \n            response = requests.post(webhook_url, json=payload, timeout=10)\n            response.raise_for_status()\n            \n        except Exception as e:\n            logger.warning(f\"Failed to send Slack alert: {e}\")\n    \n    def check_thresholds_and_alert(self, health_data: Dict[str, Any]):\n        \"\"\"Check health thresholds and send alerts if needed.\"\"\"\n        alerts = []\n        \n        # Check volume health\n        volume_health = health_data.get('volume_health', {})\n        for category, health in volume_health.items():\n            if not health.get('accessible'):\n                alerts.append({\n                    \"severity\": \"critical\",\n                    \"message\": f\"üîí Volume {category} is not accessible: {health.get('error_message')}\"\n                })\n        \n        # Check cache performance\n        cache_perf = health_data.get('cache_performance', {})\n        hit_rate = cache_perf.get('hit_rate', 0)\n        if hit_rate < 0.5:  # Less than 50% hit rate\n            alerts.append({\n                \"severity\": \"warning\",\n                \"message\": f\"üìâ Cache hit rate is low: {hit_rate:.1%}\"\n            })\n        \n        avg_exec_time = cache_perf.get('avg_execution_time_ms', 0)\n        if avg_exec_time > 5000:  # More than 5 seconds\n            alerts.append({\n                \"severity\": \"warning\", \n                \"message\": f\"‚è±Ô∏è Average query execution time is high: {avg_exec_time:.0f}ms\"\n            })\n        \n        # Check query execution\n        query_health = health_data.get('query_execution_health', {})\n        if not query_health.get('query_execution_available'):\n            alerts.append({\n                \"severity\": \"critical\",\n                \"message\": f\"‚ùå Query execution is not available: {query_health.get('error_message')}\"\n            })\n        \n        # Check Genie API\n        genie_health = health_data.get('genie_health', {})\n        if not genie_health.get('genie_api_available'):\n            alerts.append({\n                \"severity\": \"error\",\n                \"message\": f\"ü§ñ Genie API is not available: {genie_health.get('error_message')}\"\n            })\n        \n        # Send alerts\n        for alert in alerts:\n            self.send_slack_alert(alert['message'], alert['severity'])\n        \n        return alerts\n\n\ndef main():\n    \"\"\"Main monitoring job execution.\"\"\"\n    logger.info(\"Starting system monitoring job\")\n    \n    try:\n        monitor = SystemMonitor()\n        \n        # Collect all health data\n        health_data = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"volume_health\": monitor.check_volume_health(),\n            \"cache_performance\": monitor.check_cache_performance(),\n            \"query_execution_health\": monitor.check_query_execution_health(),\n            \"genie_health\": monitor.check_databricks_genie_health()\n        }\n        \n        # Check thresholds and send alerts\n        alerts = monitor.check_thresholds_and_alert(health_data)\n        \n        # Get system summary\n        system_summary = monitor.get_system_summary()\n        \n        logger.info(\n            \"System monitoring completed\",\n            alerts_sent=len(alerts),\n            overall_status=system_summary.get('overall_status')\n        )\n        \n        # Send periodic health summary (every hour)\n        current_minute = datetime.now().minute\n        if current_minute < 5:  # Send summary in first 5 minutes of each hour\n            cache_perf = health_data.get('cache_performance', {})\n            hit_rate = cache_perf.get('hit_rate', 0)\n            total_requests = cache_perf.get('total_requests_1h', 0)\n            \n            summary_msg = f\"üìä Hourly Summary - Status: {system_summary.get('overall_status', 'unknown').upper()}, Cache Hit Rate: {hit_rate:.1%}, Requests: {total_requests}\"\n            monitor.send_slack_alert(summary_msg, \"info\")\n        \n    except Exception as e:\n        error_msg = f\"‚ùå System monitoring job failed: {str(e)}\"\n        logger.error(error_msg)\n        \n        try:\n            monitor = SystemMonitor()\n            monitor.send_slack_alert(error_msg, \"critical\")\n        except:\n            pass\n        \n        raise\n\n\nif __name__ == \"__main__\":\n    main()